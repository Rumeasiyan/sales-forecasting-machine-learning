{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Techniques for Sales Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Datasets & Read all csv files\n",
    "\n",
    "files available at: https://www.kaggle.com/datasets/ndarshan2797/english-converted-datasets\n",
    "\n",
    "01. item_categories.csv - \n",
    "    item_category_name, \n",
    "    item_category_id\n",
    "\n",
    "02. items.csv - \n",
    "    item_name, \n",
    "    item_id, \n",
    "    category_id\n",
    "\n",
    "03. sales_train.csv - \n",
    "    date, \n",
    "    date_block_num, \n",
    "    shop_id, \n",
    "    item_id, \n",
    "    item_price, \n",
    "    item_cnt_day\n",
    "\n",
    "04. shops.csv - \n",
    "    shop_name, \n",
    "    shop_id\n",
    "\n",
    "05. test.csv - \n",
    "    ID, \n",
    "    shop_id, \n",
    "    item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data\n",
    "\n",
    "item_categories = pd.read_csv('./data-set/item_categories.csv')\n",
    "items = pd.read_csv('./data-set/items.csv')\n",
    "sales_train = pd.read_csv('./data-set/sales_train.csv')\n",
    "shops = pd.read_csv('./data-set/shops.csv')\n",
    "test = pd.read_csv('./data-set/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape of the data\n",
    "print(\"Shape of item_categories:\", item_categories.shape)\n",
    "print(\"Shape of items:\", items.shape)\n",
    "print(\"Shape of sales_train:\", sales_train.shape)\n",
    "print(\"Shape of shops:\", shops.shape)\n",
    "print(\"Shape of test:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the columns of the data\n",
    "print(\"\\n\\nColumns of item_categories:\\n\")\n",
    "print(item_categories.info())\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "print(\"\\n\\nColumns of items:\\n\")\n",
    "print(items.info())\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "print(\"\\n\\nColumns of sales_train:\\n\")\n",
    "print(sales_train.info())\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "print(\"\\n\\nColumns of shops:\\n\")\n",
    "print(shops.info())\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "print(\"\\n\\nColumns of test:\\n\")\n",
    "print(test.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the head of the data\n",
    "\n",
    "print(\"\\n\\nHead of item_categories:\\n\")\n",
    "print(item_categories.head())\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "print(\"\\n\\nHead of items:\\n\")\n",
    "print(items.head())\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "print(\"\\n\\nHead of sales_train:\\n\")\n",
    "print(sales_train.head())\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "print(\"\\n\\nHead of shops:\\n\")\n",
    "print(shops.head())\n",
    "\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "print(\"\\n\\nHead of test:\\n\")\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge sales_train.csv with items.csv on the \"item_id\" column\n",
    "sales_with_items = sales_train.merge(items, on='item_id', how='left')\n",
    "print(\"\\n\\nHead of sales_with_items:\\n\")\n",
    "print(sales_with_items.head(20))\n",
    "print(sales_with_items.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the result with item_categories.csv on the \"category_id\" \n",
    "sales_with_items_and_categories = sales_with_items.merge(item_categories, right_on='item_category_id', left_on='category_id', how='left')\n",
    "print(\"\\n\\nHead of sales_with_items_and_categories:\\n\")\n",
    "print(sales_with_items_and_categories.head(20))\n",
    "print(sales_with_items_and_categories.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the two columns are the same\n",
    "if sales_with_items_and_categories['item_category_id'].equals(sales_with_items_and_categories['category_id']):\n",
    "    # If they are the same, you can drop one of the columns\n",
    "    sales_with_items_and_categories.drop(columns=['item_category_id'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nHead of sales_with_items_and_categories:\\n\")\n",
    "print(sales_with_items_and_categories.head(20))\n",
    "print(sales_with_items_and_categories.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge sales_train.csv with items.csv on the \"item_id\" column\n",
    "sales_with_items = sales_train.merge(items, on='item_id', how='left')\n",
    "print(\"\\n\\nHead of sales_with_items:\\n\")\n",
    "print(sales_with_items.head(20))\n",
    "print(sales_with_items.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the result with item_categories.csv on the \"category_id\" \n",
    "sales_with_items_and_categories = sales_with_items.merge(item_categories, right_on='item_category_id', left_on='category_id', how='left')\n",
    "print(\"\\n\\nHead of sales_with_items_and_categories:\\n\")\n",
    "print(sales_with_items_and_categories.head(20))\n",
    "print(sales_with_items_and_categories.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the two columns are the same\n",
    "if sales_with_items_and_categories['item_category_id'].equals(sales_with_items_and_categories['category_id']):\n",
    "    # If they are the same, you can drop one of the columns\n",
    "    sales_with_items_and_categories.drop(columns=['item_category_id'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nHead of sales_with_items_and_categories:\\n\")\n",
    "print(sales_with_items_and_categories.head(20))\n",
    "print(sales_with_items_and_categories.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the result with shops.csv on the \"shop_id\" \n",
    "final_dataset = sales_with_items_and_categories.merge(shops, on='shop_id', how='left')\n",
    "print(\"\\n\\nHead of final_dataset:\\n\")\n",
    "print(final_dataset.head(20))\n",
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks the columns of the final dataset\n",
    "print(\"\\n\\nColumns of final_dataset:\\n\")\n",
    "print(final_dataset.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints the date and date_block_num column to check whether they are related\n",
    "columns_to_print = ['date', 'date_block_num']\n",
    "print(final_dataset[columns_to_print])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column\n",
    "final_dataset.rename(columns={'date_block_num': 'month_num'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the item_cnt_day column\n",
    "final_dataset.rename(columns={'item_cnt_day': 'item_cnt_month'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nHead of final_dataset:\\n\")\n",
    "print(final_dataset.head(20))\n",
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks the columns of the final dataset\n",
    "print(\"\\n\\nColumns of final_dataset:\\n\")\n",
    "print(final_dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning\n",
    "\n",
    "#checking for missing values\n",
    "print(\"\\n\\nMissing values in final_dataset:\\n\")\n",
    "print(final_dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for null values\n",
    "print(\"\\n\\nNull values in final_dataset:\\n\")\n",
    "print(final_dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handles the missing values in final_dataset\n",
    "final_dataset['item_name'].fillna('Unknown', inplace=True)\n",
    "final_dataset['item_category_name'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes duplicates rows in final_dataset\n",
    "final_dataset.drop_duplicates(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks and solves the data type of the columns\n",
    "print(\"\\n\\nData types of final_dataset:\\n\")\n",
    "print(final_dataset.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seems like item_cnt_month should be int64\n",
    "final_dataset['item_cnt_month'] = final_dataset['item_cnt_month'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_dataset.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints item_cnt_month column to check whether it is int64\n",
    "print(final_dataset['item_cnt_month'].head(30))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes -1 and 307980 from item_cnt_month column\n",
    "#because it is an outlier\n",
    "#and it is not possible to sell -1 and 307980 items in a day\n",
    "#because 307980 is the total number of items sold in a day\n",
    "#which means that the data is incorrect\n",
    "#and -1 is not possible\n",
    "#which means that the data is incorrect\n",
    "\n",
    "final_dataset = final_dataset[(final_dataset['item_cnt_month'] > 0) & (final_dataset['item_cnt_month'] < 307980)]\n",
    "\n",
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with the incorrect data in the item_price column\n",
    "#the item_price should not be negative\n",
    "#the item_price should not be zero\n",
    "#the item_price should not be greater than 100000\n",
    "\n",
    "final_dataset = final_dataset[(final_dataset['item_price'] > 0) & (final_dataset['item_price'] < 100000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handles special characters and formatting in the data set\n",
    "final_dataset['item_name'] = final_dataset['item_name'].str.replace('[^A-Za-z0-9А-Яа-я]+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handles noise in the data set\n",
    "\n",
    "#removes the noise in the item_name column\n",
    "final_dataset['item_name'] = final_dataset['item_name'].str.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data integration\n",
    "\n",
    "#creates a new column called revenue\n",
    "final_dataset['revenue'] = final_dataset['item_cnt_month'] * final_dataset['item_price']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nHead of final_dataset:\\n\")\n",
    "print(final_dataset.head(20))\n",
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering\n",
    "\n",
    "#creates a new column called revenue_per_item\n",
    "final_dataset['revenue_per_item'] = final_dataset['revenue'] / final_dataset['item_cnt_month']\n",
    "\n",
    "\n",
    "print(\"\\n\\nHead of final_dataset:\\n\")\n",
    "print(final_dataset.head(20))\n",
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks whether the revenue_per_item column and revenue column are the same\n",
    "\n",
    "if final_dataset['revenue_per_item'].equals(final_dataset['revenue']):\n",
    "    # If they are the same, you can drop one of the columns\n",
    "    final_dataset.drop(columns=['revenue_per_item'], inplace=True)\n",
    "\n",
    "print(\"\\n\\nHead of final_dataset:\\n\")\n",
    "print(final_dataset.head(20))\n",
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data integration and data transformation\n",
    "\n",
    "#creates a new column called date num\n",
    "final_dataset['date_num'] = final_dataset['date'].str[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nHead of final_dataset:\\n\")\n",
    "print(final_dataset.head(20))\n",
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data integration\n",
    "\n",
    "#creates a new column called year num\n",
    "final_dataset['year_num'] = final_dataset['date'].str[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nHead of final_dataset:\\n\")\n",
    "print(final_dataset.head(20))\n",
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_dataset.shape)\n",
    "print(final_dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearrange the columns\n",
    "final_dataset = final_dataset[['date', 'date_num', 'year_num', 'month_num', 'shop_id', 'shop_name', 'item_id', 'item_name', 'category_id', 'item_category_name', 'item_price', 'item_cnt_month', 'revenue']]\n",
    "\n",
    "print(final_dataset.shape)\n",
    "print(final_dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we already handled the missing and null values in the data set there is no need for imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data profiling\n",
    "\n",
    "#descriptive statistics\n",
    "print(\"\\n\\nDescriptive statistics of final_dataset:\\n\")\n",
    "print(final_dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data enrichment\n",
    "\n",
    "#creates a new column called month name\n",
    "final_dataset['month_name'] = final_dataset['month_num'].replace({0: 'January', 1: 'February', 2: 'March', 3: 'April', 4: 'May', 5: 'June', 6: 'July', 7: 'August', 8: 'September', 9: 'October', 10: 'November', 11: 'December', 12: 'January', 13: 'February', 14: 'March', 15: 'April', 16: 'May', 17: 'June', 18: 'July', 19: 'August', 20: 'September', 21: 'October', 22: 'November', 23: 'December', 24: 'January', 25: 'February', 26: 'March', 27: 'April', 28: 'May', 29: 'June', 30: 'July', 31: 'August', 32: 'September', 33: 'October'})\n",
    "\n",
    "print(\"\\n\\nHead of final_dataset:\\n\")\n",
    "print(final_dataset.head(20))\n",
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removes month_num column\n",
    "\n",
    "final_dataset.drop(columns=['month_num'], inplace=True)\n",
    "\n",
    "print(\"\\n\\nHead of final_dataset:\\n\")\n",
    "print(final_dataset.head(20))\n",
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rearange the columns\n",
    "\n",
    "final_dataset = final_dataset[['date', 'date_num', 'month_name', 'year_num', 'shop_id', 'shop_name', 'item_id', 'item_name', 'category_id', 'item_category_name', 'item_price', 'item_cnt_month', 'revenue']]\n",
    "\n",
    "print(\"\\n\\nHead of final_dataset:\\n\")\n",
    "print(final_dataset.head(20))\n",
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imputation is already performed under data preprocessing\n",
    "#outliers are handled already under data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data binning\n",
    "\n",
    "#found the bins using the following code\n",
    "print(final_dataset['item_price'].max())\n",
    "print(final_dataset['item_price'].min())\n",
    "\n",
    "#creates a new column called price range\n",
    "final_dataset['price_range'] = pd.cut(final_dataset['item_price'], bins=[-1, 100, 200, 300, 400, 500, 600, 700, 800, 900, 100000], labels=['0-100', '100-200', '200-300', '300-400', '400-500', '500-600', '600-700', '700-800', '800-900', '900-100000'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nHead of final_dataset:\\n\")\n",
    "print(final_dataset.head(20))\n",
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log transformation\n",
    "#this will help to address certain issues in the data set and it'll help to enhance the performance of the model\n",
    "\n",
    "#creates a new column called log_revenue\n",
    "final_dataset['log_revenue'] = np.log(final_dataset['revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nHead of final_dataset:\\n\")\n",
    "print(final_dataset.head(20))\n",
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding\n",
    "\n",
    "#encodes the year_num column to 0, 1, 2\n",
    "\n",
    "final_dataset['year_num'] = final_dataset['year_num'].replace({'2013': 0, '2014': 1, '2015': 2})\n",
    "\n",
    "\n",
    "print(\"\\n\\nHead of final_dataset:\\n\")\n",
    "print(final_dataset.head(20))\n",
    "print(final_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouping and aggregation\n",
    "\n",
    "#grouping the data set by shop_id and year_num\n",
    "#and aggregating the item_cnt_month column using sum\n",
    "\n",
    "grouped_by_shop_id_and_year_num = final_dataset.groupby(['shop_id', 'year_num']).agg({'item_cnt_month': 'sum'})\n",
    "\n",
    "print(\"\\n\\nHead of grouped_by_shop_id_and_year_num:\\n\")\n",
    "print(grouped_by_shop_id_and_year_num.head(60))\n",
    "print(grouped_by_shop_id_and_year_num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature split is already performed under data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling\n",
    "\n",
    "#creates a new column called scaled_revenue\n",
    "final_dataset['scaled_revenue'] = (final_dataset['revenue'] - final_dataset['revenue'].min()) / (final_dataset['revenue'].max() - final_dataset['revenue'].min())\n",
    "\n",
    "print(\"\\n\\nHead of final_dataset:\\n\")\n",
    "print(final_dataset.head(20))\n",
    "print(final_dataset.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change month_name column to numeric\n",
    "\n",
    "final_dataset['month_name'] = final_dataset['month_name'].replace({'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June':6, 'July': 7, 'August': 8, 'September': 9, 'October': 10, 'November':11, 'December': 12})\n",
    "\n",
    "print(\"\\n\\nHead of final_dataset:\\n\")\n",
    "print(final_dataset.head(20))\n",
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#correlation\n",
    "\n",
    "numeric_columns = final_dataset.select_dtypes(include=['number'])\n",
    "print(\"\\n\\nCorrelation of final_dataset:\\n\")\n",
    "print(numeric_columns.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values treatment\n",
    "\n",
    "#checks for missing values\n",
    "print(\"\\n\\nMissing values in final_dataset:\\n\")\n",
    "print(final_dataset.isnull().sum())\n",
    "\n",
    "#checks for null values\n",
    "print(\"\\n\\nNull values in final_dataset:\\n\")\n",
    "print(final_dataset.isnull().sum())\n",
    "\n",
    "#since there are no missing values and null values in the data set there is no need for imputation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier treatment\n",
    "\n",
    "#checks for outliers in the item_cnt_month column\n",
    "print(\"\\n\\nOutliers in item_cnt_month column:\\n\")\n",
    "print(final_dataset[final_dataset['item_cnt_month'] > 1000])\n",
    "\n",
    "#removes the outliers in the item_cnt_month column\n",
    "final_dataset = final_dataset[final_dataset['item_cnt_month'] < 1000]\n",
    "\n",
    "print(\"\\n\\nHead of final_dataset:\\n\")\n",
    "print(final_dataset.head(20))\n",
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Descriptive analytics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Summary Statistics\n",
    "print(\"\\nDescriptive statistics of final_dataset:\")\n",
    "print(final_dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seasonality analysis\n",
    "\n",
    "#grouping the data set by month_name\n",
    "#and aggregating the item_cnt_month column using sum\n",
    "\n",
    "grouped_by_month_name = final_dataset.groupby(['month_name']).agg({'item_cnt_month': 'sum'})\n",
    "\n",
    "print(\"\\n\\nHead of grouped_by_month_name:\\n\")\n",
    "print(grouped_by_month_name)\n",
    "print(grouped_by_month_name.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "#performing seasonal decomposition\n",
    "decomposition = sm.tsa.seasonal_decompose(grouped_by_month_name, model='additive', period=1)\n",
    "\n",
    "\n",
    "#plotting the seasonal decomposition\n",
    "fig = decomposition.plot()\n",
    "plt.show()\n",
    "\n",
    "#data visualization\n",
    "\n",
    "#plotting the item_cnt_month column\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(final_dataset['item_cnt_month'])\n",
    "plt.title('Item Count Per Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Item Count')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regulatory analytics\n",
    "\n",
    "#grouping the data set by shop_id and year_num\n",
    "#and aggregating the item_cnt_month column using sum\n",
    "\n",
    "grouped_by_shop_id_and_year_num = final_dataset.groupby(['shop_id', 'year_num']).agg({'item_cnt_month': 'sum'})\n",
    "\n",
    "print(\"\\n\\nHead of grouped_by_shop_id_and_year_num:\\n\")\n",
    "print(grouped_by_shop_id_and_year_num.head(60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable Identification\n",
    "\n",
    "# Identify numerical and categorical variables\n",
    "numerical_vars = final_dataset.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_vars = final_dataset.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# Print the list of numerical and categorical variables\n",
    "print(\"Numerical Variables:\")\n",
    "print(numerical_vars)\n",
    "\n",
    "print(\"\\nCategorical Variables:\")\n",
    "print(categorical_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#univariate analysis\n",
    "\n",
    "# for column in final_dataset.columns:\n",
    "#     # Check the data type of the variable\n",
    "#     variable_type = final_dataset[column].dtype\n",
    "    \n",
    "#     # Summary Statistics\n",
    "#     summary_stats = final_dataset[column].describe()\n",
    "    \n",
    "#     # Visualization\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "    \n",
    "#     # For numerical variables, create a histogram\n",
    "#     if variable_type in ['int64', 'float64']:\n",
    "#         sns.histplot(data=final_dataset, x=column, kde=True)\n",
    "#         plt.title(f'Distribution of {column}')\n",
    "#         plt.xlabel(column)\n",
    "#         plt.ylabel('Frequency')\n",
    "    \n",
    "#     # For categorical variables, create a bar plot\n",
    "#     else:\n",
    "#         sns.countplot(data=final_dataset, x=column)\n",
    "#         plt.title(f'Counts of {column}')\n",
    "#         plt.xlabel(column)\n",
    "#         plt.ylabel('Count')\n",
    "    \n",
    "#     plt.show()\n",
    "    \n",
    "#     # Print summary statistics\n",
    "#     print(f\"Summary Statistics for {column}:\")\n",
    "#     print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bivariate analysis\n",
    "\n",
    "var1 = 'item_price'\n",
    "var2 = 'item_cnt_month'\n",
    "\n",
    "# Check the data types of the chosen variables\n",
    "var1_type = final_dataset[var1].dtype\n",
    "var2_type = final_dataset[var2].dtype\n",
    "\n",
    "# Scatter Plot for Numerical vs. Numerical\n",
    "if var1_type in ['int64', 'float64'] and var2_type in ['int64', 'float64']:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=final_dataset, x=var1, y=var2)\n",
    "    plt.title(f'Scatter Plot: {var1} vs. {var2}')\n",
    "    plt.xlabel(var1)\n",
    "    plt.ylabel(var2)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Box Plot for Categorical vs. Numerical\n",
    "elif var1_type in ['object', 'category'] and var2_type in ['int64', 'float64']:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=final_dataset, x=var1, y=var2)\n",
    "    plt.title(f'Box Plot: {var1} vs. {var2}')\n",
    "    plt.xlabel(var1)\n",
    "    plt.ylabel(var2)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Bar Plot for Categorical vs. Categorical (if applicable)\n",
    "elif var1_type in ['object', 'category'] and var2_type in ['object', 'category']:\n",
    "    crosstab = pd.crosstab(final_dataset[var1], final_dataset[var2])\n",
    "    crosstab.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "    plt.title(f'Bar Plot: {var1} vs. {var2}')\n",
    "    plt.xlabel(var1)\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Print correlation for Numerical vs. Numerical\n",
    "if var1_type in ['int64', 'float64'] and var2_type in ['int64', 'float64']:\n",
    "    correlation = final_dataset[[var1, var2]].corr().iloc[0, 1]\n",
    "    print(f'Correlation between {var1} and {var2}: {correlation:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploratory Data Analysis (EDA)\n",
    "\n",
    "df = final_dataset.copy()\n",
    "\n",
    "# # Display basic information about the dataset\n",
    "# print(\"Dataset Overview:\")\n",
    "# print(df.info())\n",
    "\n",
    "# # Summary statistics for numerical variables\n",
    "# print(\"\\nSummary Statistics for Numerical Variables:\")\n",
    "# print(df.describe())\n",
    "\n",
    "# # Check for missing values\n",
    "# print(\"\\nMissing Values:\")\n",
    "# print(df.isnull().sum())\n",
    "\n",
    "# # Visualize data distribution using histograms for numerical variables\n",
    "# numerical_columns = ['month_name', 'year_num', 'shop_id', 'item_id', 'category_id', 'item_price', 'item_cnt_month', 'revenue', 'log_revenue', 'scaled_revenue']\n",
    "\n",
    "# for column in numerical_columns:\n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     sns.histplot(data=df, x=column, kde=True, bins=20)\n",
    "#     plt.title(f'Distribution of {column}')\n",
    "#     plt.xlabel(column)\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.show()\n",
    "\n",
    "# # Visualize relationships between variables with a correlation matrix for numerical variables\n",
    "# correlation_matrix = df[numerical_columns].corr()\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "# plt.title(\"Correlation Heatmap for Numerical Variables\")\n",
    "# plt.show()\n",
    "\n",
    "# # Explore categorical variables with bar plots\n",
    "# categorical_columns = ['shop_name', 'item_name', 'item_category_name', 'price_range']\n",
    "\n",
    "# for column in categorical_columns:\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.countplot(data=df, x=column)\n",
    "#     plt.title(f'Counts of {column}')\n",
    "#     plt.xlabel(column)\n",
    "#     plt.ylabel('Count')\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inferential analysis\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Generate a hypothetical dataset (replace this with your own data)\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(loc=70, scale=10, size=100)\n",
    "\n",
    "# Create a DataFrame from the generated data\n",
    "df = pd.DataFrame({'measurement': data})\n",
    "\n",
    "# Calculate the sample mean and standard deviation\n",
    "sample_mean = df['measurement'].mean()\n",
    "sample_std = df['measurement'].std()\n",
    "\n",
    "# Define a hypothetical population mean for comparison\n",
    "population_mean = 75 \n",
    "\n",
    "# Perform a t-test to compare the sample mean with the population mean\n",
    "t_statistic, p_value = stats.ttest_1samp(df['measurement'], population_mean)\n",
    "\n",
    "# Print results\n",
    "print(f\"Sample Mean: {sample_mean:.2f}\")\n",
    "print(f\"Sample Standard Deviation: {sample_std:.2f}\")\n",
    "print(f\"Population Mean: {population_mean}\")\n",
    "print(f\"T-Statistic: {t_statistic:.2f}\")\n",
    "print(f\"P-Value: {p_value:.4f}\")\n",
    "\n",
    "# Determine statistical significance\n",
    "alpha = 0.05  # Significance level (adjust as needed)\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: The sample mean is statistically different from the population mean.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the sample mean and the population mean.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dianostic analytics\n",
    "\n",
    "# Import libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a hypothetical dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X + 2 + np.random.randn(100, 1)\n",
    "\n",
    "# Create a DataFrame from the generated data\n",
    "df = pd.DataFrame({'X': X.flatten(), 'y': y.flatten()})\n",
    "\n",
    "# Diagnostic Plots\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qualitative analytics\n",
    "\n",
    "# 1. Count the unique values in a categorical column (e.g., item_category_name)\n",
    "category_counts = final_dataset['item_category_name'].value_counts()\n",
    "print(category_counts)\n",
    "\n",
    "# 2. Cross-tabulation to examine relationships between two categorical columns (e.g., shop_name and item_category_name)\n",
    "cross_tab = pd.crosstab(final_dataset['shop_name'], final_dataset['item_category_name'])\n",
    "print(cross_tab)\n",
    "\n",
    "# 3. Calculate the frequency of a particular category within a column\n",
    "category_frequency = (final_dataset['price_range'] == 'Low').sum()\n",
    "print(f\"Frequency of 'Low' price range: {category_frequency}\")\n",
    "\n",
    "# 4. Grouping and aggregation (e.g., average item price per category)\n",
    "average_price_per_category = final_dataset.groupby('item_category_name')['item_price'].mean()\n",
    "print(average_price_per_category)\n",
    "\n",
    "# 5. Visualizing categorical data (e.g., a bar chart of category counts)\n",
    "category_counts.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Item Category Counts')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stationarity analysis\n",
    "\n",
    "%pip install pandas numpy statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Convert the date column to datetime format\n",
    "final_dataset['date'] = pd.to_datetime(final_dataset['date'], format='%d.%m.%Y')\n",
    "\n",
    "# Group by month and aggregate data\n",
    "monthly_data = final_dataset.groupby(final_dataset['date'].dt.to_period('M')).agg({\n",
    "    'item_cnt_month': 'sum',\n",
    "    # Add other columns to aggregate as needed\n",
    "}).reset_index()\n",
    "\n",
    "# Check for stationarity using the Augmented Dickey-Fuller test\n",
    "def adf_test(timeseries):\n",
    "    result = adfuller(timeseries, autolag='AIC')\n",
    "    print('ADF Statistic:', result[0])\n",
    "    print('p-value:', result[1])\n",
    "    print('Critical Values:')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'   {key}: {value}')\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Stationary (Reject the null hypothesis)\")\n",
    "    else:\n",
    "        print(\"Non-Stationary (Fail to reject the null hypothesis)\")\n",
    "\n",
    "# Example: Check stationarity for 'item_cnt_month'\n",
    "item_cnt_month_series = monthly_data['item_cnt_month']\n",
    "\n",
    "# Plot the time series data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(item_cnt_month_series)\n",
    "plt.title('Monthly Item Count Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Item Count')\n",
    "plt.show()\n",
    "\n",
    "# Perform ADF test for stationarity\n",
    "adf_test(item_cnt_month_series)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #autocorrelation analysis\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # Set the 'date' column as the DataFrame's index\n",
    "# final_dataset.set_index('date', inplace=True)\n",
    "\n",
    "# # Sort the DataFrame by date if it's not already sorted\n",
    "# final_dataset.sort_index(inplace=True)\n",
    "\n",
    "# # Calculate the autocorrelation for the 'item_cnt_month' column\n",
    "# autocorrelation = final_dataset['item_cnt_month'].autocorr()\n",
    "\n",
    "# # Plot the autocorrelation function (ACF)\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# pd.plotting.autocorrelation_plot(final_dataset['item_cnt_month'])\n",
    "# plt.title(f'Autocorrelation for item_cnt_month (lag = 1), Autocorrelation = {autocorrelation:.2f}')\n",
    "# plt.xlabel('Lag')\n",
    "# plt.ylabel('Autocorrelation')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nHead of final_dataset:\\n\")\n",
    "print(final_dataset.head(20))\n",
    "print(final_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nHead of final_dataset:\\n\")\n",
    "print(final_dataset.head(20))\n",
    "print(final_dataset.shape)\n",
    "print(final_dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression\n",
    "\n",
    "# Create a copy of the final_dataset DataFrame\n",
    "df_lin_reg = final_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "# Feature selection and engineering\n",
    "X = final_dataset[['date_num', 'month_name', 'year_num', 'shop_id', 'item_id', 'category_id', 'item_price', 'revenue']]\n",
    "y = final_dataset['item_cnt_month']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and fit a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple linear regression\n",
    "\n",
    "#create a copy of the dataframe\n",
    "df_multi_linear = final_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm\n",
    "\n",
    "#create a copy of the dataframe\n",
    "df_svm = final_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn\n",
    "\n",
    "#create a copy of the dataframe\n",
    "df_knn = final_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "#create a copy of the dataframe\n",
    "df_decision_tree = final_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "#create a copy of the dataframe\n",
    "df_random_forest = final_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xtra tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xtra trees\n",
    "\n",
    "#create a copy of the dataframe\n",
    "df_extra_trees = final_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "#create a copy of the dataframe\n",
    "df_xgboost = final_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ridge regression\n",
    "\n",
    "#create a copy of the dataframe\n",
    "df_ridge_regression = final_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso regression\n",
    "\n",
    "#create a copy of the dataframe\n",
    "df_lasso_regression = final_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA\n",
    "\n",
    "#create a copy of the dataframe\n",
    "df_arima = final_dataset.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data Visualization\n",
    "# # Histograms for numerical columns\n",
    "# numerical_cols = final_dataset.select_dtypes(include=['number']).columns\n",
    "# for col in numerical_cols:\n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     sns.histplot(data=final_dataset, x=col, kde=True)\n",
    "#     plt.title(f'Distribution of {col}')\n",
    "#     plt.show()\n",
    "\n",
    "#     # Box plots for numerical columns\n",
    "# for col in numerical_cols:\n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     sns.boxplot(data=final_dataset, y=col)\n",
    "#     plt.title(f'Box plot of {col}')\n",
    "#     plt.show()\n",
    "\n",
    "# # Correlation Heatmap\n",
    "# correlation_matrix = numerical_cols.corr()\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "# plt.title(\"Correlation Heatmap\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
